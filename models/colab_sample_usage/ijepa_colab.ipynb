{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e393B8AJumsz"
      },
      "source": [
        "# cloning git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddEWlyOJrxah",
        "outputId": "003afb79-04c0-4ece-c2cb-0ce841e8892c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'images'...\n",
            "remote: Enumerating objects: 269, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 269 (delta 48), reused 0 (delta 0), pack-reused 144\u001b[K\n",
            "Receiving objects: 100% (269/269), 67.63 KiB | 4.51 MiB/s, done.\n",
            "Resolving deltas: 100% (107/107), done.\n",
            "/content/images\n"
          ]
        }
      ],
      "source": [
        "!git clone 'https://github.com/edwardhan925192/images.git'\n",
        "%cd images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# yaml"
      ],
      "metadata": {
        "id": "LzsrWkbta4bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "with open('/content/ijepa_image1k.yaml', 'r') as file:\n",
        "    args = yaml.safe_load(file)\n",
        "\n",
        "# -- MASK\n",
        "allow_overlap = args['mask']['allow_overlap']  # whether to allow overlap b/w context and target blocks\n",
        "patch_size = args['mask']['patch_size']  # patch-size for model training\n",
        "num_enc_masks = args['mask']['num_enc_masks']  # number of context blocks\n",
        "min_keep = args['mask']['min_keep']  # min number of patches in context block\n",
        "enc_mask_scale = args['mask']['enc_mask_scale']  # scale of context blocks\n",
        "num_pred_masks = args['mask']['num_pred_masks']  # number of target blocks\n",
        "pred_mask_scale = args['mask']['pred_mask_scale']  # scale of target blocks\n",
        "aspect_ratio = args['mask']['aspect_ratio']  # aspect ratio of target blocks\n",
        "# --\n",
        "\n",
        "# -- OPTIMIZATION\n",
        "ema = args['optimization']['ema']\n",
        "ipe_scale = args['optimization']['ipe_scale']  # scheduler scale factor (def: 1.0)\n",
        "wd = float(args['optimization']['weight_decay'])\n",
        "num_epochs = args['optimization']['epochs']\n",
        "lr = args['optimization']['lr']"
      ],
      "metadata": {
        "id": "F_4tnKcsdrC_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading check points"
      ],
      "metadata": {
        "id": "JDer2LNovYUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(\n",
        "    device,\n",
        "    r_path,\n",
        "    x_encoder,\n",
        "    predictor,\n",
        "    y_encoder,\n",
        "    x_optimizer,\n",
        "    pred_optimizer\n",
        "\n",
        "):\n",
        "    try:\n",
        "      # -- saved dir\n",
        "      checkpoint = torch.load(r_path, map_location=torch.device('cpu'))\n",
        "      epoch = checkpoint['epoch']\n",
        "\n",
        "      # -- loading x_encoder\n",
        "      pretrained_dict = checkpoint['x_encoder']\n",
        "      x_encoder.load_state_dict(pretrained_dict)\n",
        "\n",
        "      # -- loading predictor\n",
        "      pretrained_dict = checkpoint['predictor']\n",
        "      predictor.load_state_dict(pretrained_dict)\n",
        "\n",
        "      # -- loading y_encoder\n",
        "      if y_encoder is not None:\n",
        "          print(list(checkpoint.keys()))\n",
        "          pretrained_dict = checkpoint['y_encoder']\n",
        "          y_encoder.load_state_dict(pretrained_dict)\n",
        "\n",
        "      # -- loading optimizer\n",
        "      pred_optimizer.load_state_dict(checkpoint['pred_optimizer'])\n",
        "      x_optimizer.load_state_dict(checkpoint['x_optimizer'])\n",
        "\n",
        "    except Exception as e:\n",
        "      epoch = 0\n",
        "\n",
        "    return x_encoder, predictor, y_encoder, x_optimizer, pred_optimizer,  epoch"
      ],
      "metadata": {
        "id": "LmFyjepNvZgz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "hwHVvTTJz5HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Example usage\n",
        "use_bfloat16 = False # bool\n",
        "num_epochs = 110\n",
        "checkpoint_freq = 5\n",
        "base_directory = '/content/drive/MyDrive/data/ijepa weights'  # where the check points are stored and loading from\n",
        "\n",
        "# -- loading path\n",
        "load_model = True\n",
        "load_path = '/content/drive/MyDrive/data/ijepa weights/-latest.pth'"
      ],
      "metadata": {
        "id": "lRX0P_GT1DnB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.ijepa import VisionTransformerPredictor, VisionTransformer\n",
        "import torch.nn.functional as F\n",
        "from utils.masks.mask_application import apply_masks\n",
        "from utils.tensors import repeat_interleave_batch\n",
        "import os\n",
        "# -- 0. datasets\n",
        "from utils.masks.maskcollator_vit import MaskCollator\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "root_dir = '/content/drive/MyDrive/data/vegi picture'\n",
        "\n",
        "# -- loading\n",
        "dataset = datasets.ImageFolder(root = root_dir, transform=transform)\n",
        "\n",
        "# -- 1. dataloader\n",
        "collator = MaskCollator()\n",
        "unsupervised_loader = DataLoader(dataset, batch_size=20, shuffle=True, collate_fn=collator)"
      ],
      "metadata": {
        "id": "562Csslk6bZJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbJtx_eatsLD"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from images.scheduler.scheduler import SchedulerManager\n",
        "\n",
        "# -- Check CUDA\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "scaler = GradScaler()\n",
        "\n",
        "# -- model init\n",
        "x_encoder = VisionTransformer()\n",
        "y_encoder = VisionTransformer()\n",
        "num_patches = 196\n",
        "predictor = VisionTransformerPredictor(num_patches = num_patches)\n",
        "\n",
        "# -- load models\n",
        "x_encoder.to(device)\n",
        "y_encoder.to(device)\n",
        "predictor.to(device)\n",
        "\n",
        "# -- optimizer\n",
        "x_optimizer = torch.optim.Adam(x_encoder.parameters(), lr=lr, weight_decay = wd)\n",
        "y_optimizer = torch.optim.Adam(y_encoder.parameters(), lr=lr, weight_decay = wd)\n",
        "pred_optimizer = torch.optim.Adam(predictor.parameters(), lr=lr, weight_decay = wd)\n",
        "\n",
        "# -- lr scheduler\n",
        "scheduler_manager = SchedulerManager()\n",
        "x_scheduler = scheduler_manager.initialize_scheduler(x_optimizer, 'CosineAnnealingWarmRestarts')\n",
        "pred_scheduler = scheduler_manager.initialize_scheduler(pred_optimizer, 'CosineAnnealingWarmRestarts')\n",
        "\n",
        "# -- momentum scheduler\n",
        "ipe = len(unsupervised_loader)\n",
        "momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
        "                      for i in range(int(ipe*num_epochs*ipe_scale)+1))\n",
        "\n",
        "def save_checkpoint(epoch, checkpoint_freq, base_directory='/content/drive/MyDrive/data/ijepa weights'):\n",
        "\n",
        "    # Latest checkpoint path\n",
        "    latest_path = os.path.join(base_directory, '-latest.pth')\n",
        "\n",
        "    # Save dictionary\n",
        "    save_dict = {\n",
        "        'x_encoder': x_encoder.state_dict(),\n",
        "        'y_encoder': y_encoder.state_dict(),\n",
        "        'predictor': predictor.state_dict(),\n",
        "        'x_optimizer': x_optimizer.state_dict(),\n",
        "        'pred_optimizer': pred_optimizer.state_dict(),\n",
        "        'scaler': None if scaler is None else scaler.state_dict(),\n",
        "        'epoch': epoch\n",
        "    }\n",
        "\n",
        "    # -- Always update the latest checkpoint\n",
        "    torch.save(save_dict, latest_path)\n",
        "\n",
        "    # -- Checkpoint frequency updates\n",
        "    if (epoch + 1) % checkpoint_freq == 0:\n",
        "\n",
        "        # -- Checkpoint path for specific epoch\n",
        "        checkpoint_path = os.path.join(base_directory, f'-ep{epoch + 1}.pth')\n",
        "        torch.save(save_dict, checkpoint_path)\n",
        "\n",
        "# ------------- Begin loading ------------- #\n",
        "if load_model == True:\n",
        "    encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
        "        device=device,\n",
        "        r_path=load_path,\n",
        "        x_encoder = x_encoder,\n",
        "        predictor=predictor,\n",
        "        y_encoder=y_encoder,\n",
        "        x_optimizer = x_optimizer,\n",
        "        pred_optimizer = pred_optimizer\n",
        "        )\n",
        "\n",
        "    # -- momentum scheduler\n",
        "    for _ in range(start_epoch*ipe):\n",
        "      next(momentum_scheduler)\n",
        "\n",
        "    # -- currently its set to be updated every epoch\n",
        "    for _ in range(start_epoch):\n",
        "      x_scheduler.step()\n",
        "      pred_scheduler.step()\n",
        "# ------------- End loading ------------- #\n",
        "\n",
        "\n",
        "# ------------- training step ------------- #\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "  x_encoder.train()\n",
        "  predictor.train()\n",
        "\n",
        "  # -- loader\n",
        "  for itr, (udata, masks_enc, masks_pred) in enumerate(unsupervised_loader):\n",
        "    def load_imgs():\n",
        "\n",
        "        # -- unsupervised imgs\n",
        "        imgs = udata[0].to(device, non_blocking=True)\n",
        "        masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
        "        masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
        "        return (imgs, masks_1, masks_2)\n",
        "    imgs, masks_enc, masks_pred = load_imgs()\n",
        "\n",
        "    def train_step():\n",
        "        # -- return masked target tokens\n",
        "        def forward_target():\n",
        "            with torch.no_grad():\n",
        "                h = y_encoder(imgs)\n",
        "                h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
        "                B = len(h)\n",
        "                # -- create targets (masked regions of h)\n",
        "                h = apply_masks(h, masks_pred)\n",
        "                h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
        "                return h\n",
        "\n",
        "        # -- return masked encoded tokens\n",
        "        def forward_context():\n",
        "            z = x_encoder(imgs, masks_enc)\n",
        "            z = predictor(z, masks_enc, masks_pred)\n",
        "            return z\n",
        "\n",
        "        def loss_fn(z, h):\n",
        "            loss = F.smooth_l1_loss(z, h)\n",
        "            return loss\n",
        "\n",
        "        # -- step 1. Forward\n",
        "        with torch.cuda.amp.autocast(dtype=torch.bfloat16, enabled=use_bfloat16):\n",
        "            h = forward_target()\n",
        "            z = forward_context()\n",
        "            loss = loss_fn(z, h)\n",
        "\n",
        "        # -- step 2. Backward & step\n",
        "        # -- scaler\n",
        "        if use_bfloat16:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(x_optimizer)\n",
        "            scaler.step(pred_optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            x_optimizer.step()\n",
        "            pred_optimizer.step()\n",
        "\n",
        "        # -- momentum update of y_encoder\n",
        "        with torch.no_grad():\n",
        "            m = next(momentum_scheduler)\n",
        "            for param_q, param_k in zip(x_encoder.parameters(), y_encoder.parameters()):\n",
        "                param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
        "\n",
        "        print(f'LOSS OF {epoch+1}: {loss}')\n",
        "\n",
        "    train_step()\n",
        "\n",
        "  # -- lr scheduler step\n",
        "  x_scheduler.step()\n",
        "  pred_scheduler.step()\n",
        "\n",
        "  # -- saving every epoch\n",
        "  save_checkpoint(epoch + 1, checkpoint_freq, base_directory)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
