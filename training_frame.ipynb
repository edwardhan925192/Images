{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# check points"
      ],
      "metadata": {
        "id": "MkHfPovGA_y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "def load_checkpoint(r_path, model, optimizer=None, scheduler=None):\n",
        "\n",
        "    if not os.path.isfile(r_path):\n",
        "        print(f\"No checkpoint found at '{r_path}'\")\n",
        "        return model, optimizer, scheduler, 0\n",
        "\n",
        "    print(f\"Loading checkpoint '{r_path}'\")\n",
        "    checkpoint = torch.load(r_path, map_location='cpu')\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    epoch = checkpoint.get('epoch', 0)\n",
        "\n",
        "    if scheduler is not None:\n",
        "        for _ in range(epoch):\n",
        "            scheduler.step()\n",
        "\n",
        "    return model, optimizer, scheduler, epoch\n",
        "\n",
        "def save_checkpoint(model, epoch, tag, base_directory, optimizer=None, current_val_score=None, best_scores=None, checkpoint_freq=1):\n",
        "    if not os.path.exists(base_directory):\n",
        "        os.makedirs(base_directory)\n",
        "\n",
        "    save_dict = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'epoch': epoch\n",
        "    }\n",
        "\n",
        "    if optimizer is not None:\n",
        "        save_dict['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "    if current_val_score is not None:\n",
        "        # Initialize best_scores if it's None\n",
        "        if best_scores is None:\n",
        "            best_scores = [(float('inf'), ''), (float('inf'), '')]  # (score, filename)\n",
        "\n",
        "        # Insert new score and sort\n",
        "        best_scores.append((current_val_score, f'{tag}_checkpoint_epoch_{epoch + 1}_val{current_val_score:.4f}.pth'))\n",
        "        best_scores.sort(key=lambda x: x[0])\n",
        "\n",
        "        # Keep only the top 2 scores\n",
        "        best_scores = best_scores[:2]\n",
        "\n",
        "        # Save the model if it's one of the top 2\n",
        "        for score, filename in best_scores:\n",
        "            if score == current_val_score:\n",
        "                save_path = os.path.join(base_directory, filename)\n",
        "                torch.save(save_dict, save_path)\n",
        "                break\n",
        "\n",
        "    # Regular checkpoint updates\n",
        "    if (epoch + 1) % checkpoint_freq == 0:\n",
        "        checkpoint_path = os.path.join(base_directory, f'{tag}_checkpoint_epoch_{epoch + 1}.pth')\n",
        "        torch.save(save_dict, checkpoint_path)\n",
        "\n",
        "    return best_scores"
      ],
      "metadata": {
        "id": "C712rXVb8g_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gradient accumulation step"
      ],
      "metadata": {
        "id": "f70w029PA94h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accumulation_steps = 4  # This means the effective batch size is batch_size * accumulation_steps\n",
        "\n",
        "# Start the training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        # Scales loss as per the number of accumulation steps\n",
        "        loss = loss / accumulation_steps"
      ],
      "metadata": {
        "id": "g67zMclZA8NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaOHOjIFtBL4"
      },
      "outputs": [],
      "source": [
        "# -- torch vision transforms ( needed to be included )\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "import copy\n",
        "import torchvision.ops\n",
        "from images.scheduler.scheduler import SchedulerManager\n",
        "from images.datasets.singlefolder_dataset import SingleFolderDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ================ INIT ================= #\n",
        "learning_rate = 0.001\n",
        "scheduler_config = {'T_0': 100, 'T_mult': 1, 'eta_min': 0.0001}\n",
        "scheduler = 'CosineAnnealingWarmRestarts'\n",
        "\n",
        "def initialization(learning_rate, model_class, scheduler, sceduler_config, criterion):\n",
        "  model = model_class().to(device) # -- model = JNet().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  scheduler_manager = SchedulerManager()\n",
        "  scheduler_manager.configs[scheduler] = scheduler_config # -- config update\n",
        "  scheduler = scheduler_manager.initialize_scheduler(optimizer, 'CosineAnnealingWarmRestarts')\n",
        "  criterion = nn.CrossEntropyLoss()  # -- criterion\n",
        "  return model, optimizer, scheduler, criterion\n",
        "\n",
        "# ================ LOADING STEP ================= #\n",
        "load_dir = ''\n",
        "load_states = True # -- bool\n",
        "\n",
        "def loading_states(load_dir, model, optimizer, scheduler):\n",
        "  model, optimizer, scheduler, start_epoch = load_checkpoint(load_dir, model, optimizer, scheduler)\n",
        "  return model, optimizer, scheduler, start_epoch\n",
        "\n",
        "# ================ DATA PREP ================= #\n",
        "train_df =\n",
        "val_df =\n",
        "collater = custom_collate_fn # or False\n",
        "batch_sizes = 24\n",
        "shuffle = False\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def dataset_prep(train_df, val_df, collater, batch_sizes, shuffle, transform ):\n",
        "    # -- target col\n",
        "    def generate_target_columns(start, end):\n",
        "      return [str(i) for i in range(start, end + 1)]\n",
        "    target_columns = generate_target_columns(1, 16)\n",
        "    # --\n",
        "\n",
        "    # -- train\n",
        "    train_dataset = DataFrameDataset_custom(train_df, 'img_path', target_columns, transform=transform)\n",
        "\n",
        "    if collater:\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_sizes, shuffle=shuffle, collate_fn=collater)\n",
        "    else:\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_sizes, shuffle=shuffle)\n",
        "\n",
        "    # -- validation\n",
        "    val_dataset = DataFrameDataset_custom(val_df, 'img_path', target_columns, transform=transform)\n",
        "\n",
        "    if collater:\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=batch_sizes, shuffle=shuffle, collate_fn=collater)\n",
        "    else:\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=batch_sizes, shuffle=shuffle)\n",
        "\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "\n",
        "# ================ TRAINING STEP ================= #\n",
        "num_epochs =\n",
        "\n",
        "def training_step(model, scheduler, train_dataloader, optimizer, criterion = None):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (batch_data, batch_target) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
        "        batch_data, batch_target = batch_data.to(device), batch_target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_data)\n",
        "\n",
        "        batch_target = batch_target.long()  # -- convert to long\n",
        "        loss = criterion(outputs, batch_target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_training_loss = total_loss / len(train_dataloader)\n",
        "    scheduler.step() # -- scheduler step\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_training_loss}\")\n",
        "\n",
        "  return model, average_training_loss, num_epochs, optimizer\n",
        "\n",
        "# ================ TRAINING STEP ================= #\n",
        "def validation_step(model, val_dataloader, criterion = None):\n",
        "  model.eval()\n",
        "  total_val_loss = 0\n",
        "  val_loss = 0\n",
        "  with torch.no_grad():\n",
        "      for batch_data, batch_target in val_dataloader:\n",
        "          batch_data, batch_target = batch_data.to(device), batch_target.to(device)\n",
        "          outputs = model(batch_data)\n",
        "\n",
        "          batch_target = batch_target.long()  # Convert target labels to torch.long\n",
        "          loss = criterion(outputs, batch_target)\n",
        "          val_loss += loss.item()\n",
        "\n",
        "  total_val_loss = val_loss / len(val_dataloader)\n",
        "  print(f\"Average Validation Loss: {total_val_loss}\")\n",
        "\n",
        "  return total_val_loss\n",
        "\n",
        "# ================ CHECK POINTS ================= #\n",
        "base_directory = 'directory'\n",
        "tag = 'name of the model'\n",
        "\n",
        "def check_points(model, epoch, tag, base_directory, optimizer, val_score, best_scores = None):\n",
        "  best_scores = save_checkpoint(model, epoch, tag, base_directory, optimizer, val_score, best_scores, checkpoint_freq=1)\n",
        "  return best_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- init\n",
        "model, optimizer, scheduler, start_epoch_t = initialization(learning_rate, model_class, scheduler, sceduler_config, criterion)\n",
        "\n",
        "# -- load\n",
        "if load_states:\n",
        "  model_t, optimizer, scheduler_t, start_epoch = loading_states(load_dir, model, optimizer, scheduler)\n",
        "\n",
        "# -- data\n",
        "train_dataloader, val_dataloader = dataset_prep(train_df, val_df, collater, batch_sizes, shuffle, transform )\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "  # -- training\n",
        "  model, training_loss, num_epochs, optimizer =  training_step(model, scheduler, train_dataloader, optimizer, criterion = None)\n",
        "\n",
        "  # -- validation\n",
        "  val_loss = validation_step(model, val_dataloader, criterion = None)\n",
        "\n",
        "  # -- saving check points\n",
        "  best_scores = check_points(model, epoch, tag, base_directory, optimizer, val_loss, best_scores = None)\n",
        ""
      ],
      "metadata": {
        "id": "-mf8TSWBiXK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}